{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.ops.boxes import _box_inter_union\n",
        "from torchvision.ops import generalized_box_iou_loss"
      ],
      "metadata": {
        "id": "H01XWrna8cAB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBiXbgQo8L3e",
        "outputId": "2c3998e8-b45d-457e-b939-1d25747afa7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CircleDetection'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 29 (delta 6), reused 24 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (29/29), 220.11 KiB | 8.80 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tcotte/CircleDetection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(os.path.join('/content', 'dataset'))\n",
        "os.makedirs(os.path.join('/content', 'dataset', 'train'))\n",
        "os.makedirs(os.path.join('/content', 'dataset', 'val'))"
      ],
      "metadata": {
        "id": "Ks5qYoal8UVY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/CircleDetection/dataset_creation/easy_circle.py --path /content/dataset/train --nb_samples 5000\n",
        "!python /content/CircleDetection/dataset_creation/easy_circle.py --path /content/dataset/val --nb_samples 500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt37ZYie9ESZ",
        "outputId": "d683f051-0488-45f5-bfef-fcb2bba1cffa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Directories creation\n",
            "[INFO] Dataset creation\n",
            "100% 4999/4999 [00:01<00:00, 3261.15it/s]\n",
            "[INFO] Directories creation\n",
            "[INFO] Dataset creation\n",
            "100% 499/499 [00:00<00:00, 3373.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CircleDetection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bqgenei9X1Z",
        "outputId": "6c2330a7-2fb7-43db-e7ca-14caf5e215ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CircleDetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def giou_loss(input_boxes, target_boxes, eps=1e-7):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_boxes: Tensor of shape (N, 4) or (4,).\n",
        "        target_boxes: Tensor of shape (N, 4) or (4,).\n",
        "        eps (float): small number to prevent division by zero\n",
        "    \"\"\"\n",
        "    inter, union = _box_inter_union(input_boxes, target_boxes)\n",
        "    iou = inter / union\n",
        "\n",
        "    # area of the smallest enclosing box\n",
        "    min_box = torch.min(input_boxes, target_boxes)\n",
        "    max_box = torch.max(input_boxes, target_boxes)\n",
        "    area_c = (max_box[:, 2] - min_box[:, 0]) * (max_box[:, 3] - min_box[:, 1])\n",
        "\n",
        "    giou = iou - ((area_c - union) / (area_c + eps))\n",
        "\n",
        "    loss = 1 - giou\n",
        "\n",
        "    return loss.sum()\n",
        "\n",
        "\n",
        "class GIoULoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GIoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        return giou_loss(input_boxes=predictions, target_boxes=target, eps=1e-7)\n",
        "\n",
        "from torchvision.ops import distance_box_iou_loss\n",
        "\n",
        "\n",
        "\n",
        "class DIoULoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "      super(DIoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "      return distance_box_iou_loss(boxes1=target, boxes2=predictions, reduction=\"mean\")"
      ],
      "metadata": {
        "id": "cqHPDdTEBh_t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_bboxes(bboxes):\n",
        "    for box in bboxes:\n",
        "        if box[0] > box[2]:\n",
        "            box[2] = box[0]\n",
        "        if box[1] > box[3]:\n",
        "            box[3] = box[1]\n",
        "    return bboxes\n"
      ],
      "metadata": {
        "id": "fV7RrMQPYP07"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params= resnet.state_dict()\n",
        "list_layers = list(params.keys())"
      ],
      "metadata": {
        "id": "VD2F2MSk868u"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_net_parameters(net):\n",
        "    for name, para in net.named_parameters():\n",
        "        print(\"-\"*20)\n",
        "        print(f\"name: {name}\")\n",
        "        print(\"values: \")\n",
        "        print(param.requires_grad)"
      ],
      "metadata": {
        "id": "X8b2buMY_2JF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in resnet.named_parameters():\n",
        "  if name in list_layers[-5:]:\n",
        "    print(\"yes\")\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "\n",
        "print_net_parameters(resnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V2fD4Ce_3lE",
        "outputId": "421ae3f1-30af-4c7b-8e18-b02bbf0c7bfe"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "yes\n",
            "--------------------\n",
            "name: conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.downsample.0.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.downsample.1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.0.downsample.1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.1.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer1.2.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.downsample.0.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.downsample.1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.0.downsample.1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.1.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.2.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer2.3.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.downsample.0.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.downsample.1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.0.downsample.1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.1.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.2.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.3.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.4.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer3.5.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.downsample.0.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.downsample.1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.0.downsample.1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.1.bn3.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.conv1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.bn1.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.bn1.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.conv2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.bn2.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.bn2.bias\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.conv3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.bn3.weight\n",
            "values: \n",
            "True\n",
            "--------------------\n",
            "name: layer4.2.bn3.bias\n",
            "values: \n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet50, resnet18, vgg16\n",
        "from tqdm import tqdm\n",
        "\n",
        "from metrics import batch_iou\n",
        "from model.object_detector import ObjectDetector\n",
        "from torch_datasets import CustomImageDataset\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
        "# specify ImageNet mean and standard deviation\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD = [0.229, 0.224, 0.225]\n",
        "# initialize our initial learning rate, number of epochs to train\n",
        "# for, and the batch size\n",
        "INIT_LR = 1e-5\n",
        "NUM_EPOCHS = 500\n",
        "BATCH_SIZE = 128\n",
        "# specify the loss weights\n",
        "LABELS = 1.0\n",
        "BBOX = 1.0\n",
        "\n",
        "model_name = \"500_epochs\"\n",
        "\n",
        "CLASSES = [\"Circle\"]\n",
        "\n",
        "bbox_format = 'albumentations'\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "     A.Equalize(mode='cv', by_channels=True, mask=None, p=0.5),\n",
        "     A.HorizontalFlip(p=0.5),\n",
        "     A.VerticalFlip(p=0.5),\n",
        "     A.RandomRotate90(p=0.5),\n",
        "     # A.Rotate(limit=90, p=0.5, border_mode=0, rotate_method=\"ellipse\"),\n",
        "     A.CLAHE(p=0.5),\n",
        "     A.OneOf([\n",
        "         A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "         A.GridDistortion(p=0.5),\n",
        "     ], p=0.0),\n",
        "     A.Normalize(always_apply=True),\n",
        "     ToTensorV2()],\n",
        "    bbox_params=A.BboxParams(format=bbox_format, label_fields=['category_ids']),\n",
        ")\n",
        "\n",
        "test_transform = A.Compose([\n",
        "    A.Normalize(always_apply=True),\n",
        "    ToTensorV2()],\n",
        "    bbox_params=A.BboxParams(format=bbox_format, label_fields=['category_ids'])\n",
        ")\n",
        "\n",
        "train_dataset = CustomImageDataset(\n",
        "    img_dir=r\"/content/dataset/train/img\",\n",
        "    label_dir=r\"/content/dataset/train/labels\",\n",
        "    transform=train_transform)\n",
        "\n",
        "val_dataset = CustomImageDataset(\n",
        "    img_dir=r\"/content/dataset/val/img\",\n",
        "    label_dir=r\"/content/dataset/val/labels\",\n",
        "    transform=test_transform)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "print(\"[INFO] total training samples: {}...\".format(len(train_dataset)))\n",
        "print(\"[INFO] total test samples: {}...\".format(len(val_dataset)))\n",
        "# calculate steps per epoch for training and validation set\n",
        "trainSteps = math.ceil(len(train_dataset) / BATCH_SIZE)\n",
        "valSteps = math.ceil(len(val_dataset) / BATCH_SIZE)\n",
        "# create data loaders\n",
        "trainLoader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=os.cpu_count(), pin_memory=PIN_MEMORY)\n",
        "testLoader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                        num_workers=os.cpu_count(), pin_memory=PIN_MEMORY)\n",
        "\n",
        "# Network\n",
        "# load the ResNet50 network\n",
        "backbone = resnet18(pretrained=False)\n",
        "# backbone = vgg16(pretrained=False)\n",
        "\n",
        "# freeze all ResNet50 layers so they will *not* be updated during the\n",
        "# training process\n",
        "params= backbone.state_dict()\n",
        "list_layers = list(params.keys())\n",
        "for name, param in backbone.named_parameters():\n",
        "  if name in list_layers[0:]:\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create our custom object detector model and flash it to the current\n",
        "# device\n",
        "objectDetector = ObjectDetector(backbone, len(CLASSES))\n",
        "objectDetector = objectDetector.to(DEVICE)\n",
        "# define our loss functions\n",
        "\n",
        "classLossFunc = CrossEntropyLoss()\n",
        "\n",
        "bboxLossFunc = DIoULoss()\n",
        "# initialize the optimizer, compile the model, and show the model\n",
        "# summary\n",
        "opt = optim.Adam(objectDetector.parameters(), lr=INIT_LR)\n",
        "\n",
        "# initialize a dictionary to store training history\n",
        "H = {\"total_train_loss\": [], \"total_val_loss\": [], \"train_class_acc\": [],\n",
        "      \"val_class_acc\": [], \"train_iou\": [], \"val_iou\": []}\n",
        "\n",
        "# loop over epochs\n",
        "print(\"[INFO] training the network...\")\n",
        "startTime = time.time()\n",
        "for e in tqdm(range(NUM_EPOCHS)):\n",
        "    # set the model in training mode\n",
        "    objectDetector.train()\n",
        "    # initialize the total training and validation loss\n",
        "    totalTrainLoss = 0\n",
        "    totalValLoss = 0\n",
        "    # initialize the number of correct predictions in the training\n",
        "    # and validation step\n",
        "    trainCorrect = 0\n",
        "    valCorrect = 0\n",
        "\n",
        "    train_iou = 0\n",
        "    val_iou = 0\n",
        "\n",
        "    # loop over the training set\n",
        "    for (images, labels, bboxes, filenames) in trainLoader:\n",
        "        # send the input to the device\n",
        "        labels = torch.Tensor(labels)\n",
        "        # bboxes = torch.stack(bboxes, dim=1)\n",
        "\n",
        "        bboxes = bboxes.to(torch.float32)\n",
        "        bboxes = torch.squeeze(bboxes, 1)\n",
        "\n",
        "        (images, labels, bboxes) = (images.to(DEVICE),\n",
        "                                    labels.to(DEVICE), bboxes.to(DEVICE))\n",
        "        # perform a forward pass and calculate the training loss\n",
        "        opt.zero_grad()\n",
        "        predictions = objectDetector(images)\n",
        "        bboxLoss = bboxLossFunc(predictions =predictions[0], target=bboxes)\n",
        "\n",
        "        totalLoss = BBOX * bboxLoss\n",
        "        totalLoss = totalLoss.to(torch.float)\n",
        "\n",
        "        # zero out the gradients, perform the backpropagation step,\n",
        "        # and update the weights\n",
        "\n",
        "        totalLoss.backward()\n",
        "        opt.step()\n",
        "        # add the loss to the total training loss so far and\n",
        "        # calculate the number of correct predictions\n",
        "        train_iou += batch_iou(a=predictions[0].detach().cpu().numpy(), b=bboxes.cpu().numpy()).sum() / len(bboxes)\n",
        "        totalTrainLoss += totalLoss\n",
        "        trainCorrect += (predictions[1].argmax(1) == labels).type(torch.float).sum().item()\n",
        "\n",
        "    # switch off autograd\n",
        "    with torch.no_grad():\n",
        "        # set the model in evaluation mode\n",
        "        objectDetector.eval()\n",
        "        # loop over the validation set\n",
        "        for (images, labels, bboxes, _) in testLoader:\n",
        "            # send the input to the device\n",
        "            labels = torch.Tensor(labels)\n",
        "            bboxes = torch.squeeze(bboxes, 1)\n",
        "\n",
        "            # bboxes = torch.stack(bboxes, dim=1)\n",
        "            (images, labels, bboxes) = (images.to(DEVICE),\n",
        "                                        labels.to(DEVICE), bboxes.to(DEVICE))\n",
        "            # make the predictions and calculate the validation loss\n",
        "            predictions = objectDetector(images)\n",
        "            bboxLoss = bboxLossFunc(predictions[0], bboxes)\n",
        "            totalLoss = BBOX * bboxLoss\n",
        "            totalValLoss += totalLoss\n",
        "            # calculate the number of correct predictions\n",
        "            val_iou += batch_iou(a=predictions[0].detach().cpu().numpy(), b=bboxes.cpu().numpy()).sum() / len(\n",
        "                bboxes)\n",
        "            valCorrect += (predictions[1].argmax(1) == labels).type(torch.float).sum().item()\n",
        "\n",
        "    # calculate the average training and validation loss\n",
        "    avgTrainLoss = totalTrainLoss / trainSteps\n",
        "    avgValLoss = totalValLoss / valSteps\n",
        "    # calculate the training and validation accuracy\n",
        "    trainCorrect = trainCorrect / len(train_dataset)\n",
        "    valCorrect = valCorrect / len(val_dataset)\n",
        "    # update our training history\n",
        "    H[\"total_train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "    H[\"train_class_acc\"].append(trainCorrect)\n",
        "    H[\"total_val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "    H[\"val_class_acc\"].append(valCorrect)\n",
        "    H[\"train_iou\"].append(train_iou/trainSteps)\n",
        "    H[\"val_iou\"].append(val_iou/valSteps)\n",
        "    # print the model training and validation information\n",
        "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
        "    print(\"Train loss: {:.6f}, Train accuracy: {:.8f}\".format(\n",
        "        avgTrainLoss, train_iou/trainSteps))\n",
        "    print(\"Val loss: {:.6f}, Val accuracy: {:.8f}\".format(\n",
        "        avgValLoss, val_iou/valSteps))\n",
        "    endTime = time.time()\n",
        "    print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
        "        endTime - startTime))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving object detector model...\")\n",
        "# torch.save(objectDetector, MODEL_PATH)\n",
        "# serialize the label encoder to disk\n",
        "print(\"[INFO] saving label encoder...\")\n",
        "torch.save(objectDetector, os.path.join(\"trained_models\", model_name + '.pt'))\n",
        "# f = open(LE_PATH, \"wb\")\n",
        "# f.write(pickle.dumps(le))\n",
        "# f.close()\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(H[\"total_train_loss\"], label=\"total_train_loss\")\n",
        "plt.plot(H[\"total_val_loss\"], label=\"total_val_loss\")\n",
        "plt.plot(H[\"train_iou\"], label=\"train_acc_iou\")\n",
        "plt.plot(H[\"val_iou\"], label=\"val_acc_iou\")\n",
        "plt.title(\"Total Training Loss and Classification Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "# save the training plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "SYH5w_xD-WWf",
        "outputId": "677cd495-6695-4e0f-84db-4e459702366c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] total training samples: 4999...\n",
            "[INFO] total test samples: 499...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-da7295bc5a2d>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# create our custom object detector model and flash it to the current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mobjectDetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0mobjectDetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjectDetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# define our loss functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CircleDetection/model/object_detector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_model, num_classes)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         self.regressor = Sequential(\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VGG' object has no attribute 'fc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"trained_models\""
      ],
      "metadata": {
        "id": "cqkd0C8z4hrJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(objectDetector, os.path.join(\"trained_models\", model_name + '.pt'))"
      ],
      "metadata": {
        "id": "bSDn1E5m4fRx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from metrics import get_iou\n",
        "\n",
        "val_dataset = CustomImageDataset(\n",
        "    img_dir=r\"/content/dataset/val/img\",\n",
        "    label_dir=r\"/content/dataset/val/labels\", transform=test_transform)\n",
        "\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "idx = 128\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "model = torch.load(r\"/content/CircleDetection/trained_models/500_epochs.pt\")\n",
        "model.eval()\n",
        "model.to('cpu')\n",
        "\n",
        "avg_iou = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # print(train_dataset[0][0])\n",
        "    sample = val_dataset[idx]\n",
        "    image = sample[0].type(torch.FloatTensor)\n",
        "\n",
        "    image = torch.unsqueeze(image, 0)\n",
        "\n",
        "    output = model(image)[0]\n",
        "    \n",
        "    # iou = get_iou(output[0][0], sample[2][0])\n",
        "    # print(sample[-1], iou)\n",
        "    # avg_iou += iou\n",
        "\n",
        "\n",
        "\n",
        "    image = cv2.imread(os.path.join(r\"/content/dataset/val/img\", val_dataset[idx][-1]))\n",
        "\n",
        "    coords = 128*output[0]\n",
        "    coords = np.round(coords.numpy())\n",
        "    coords = [int(x) for x in coords]\n",
        "\n",
        "    cv2.rectangle(image, (coords[0], coords[1]), (coords[2], coords[3]), (0, 255, 255), 1)\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "cWFV57bja79Y",
        "outputId": "05a06b75-e7d0-4d3b-b29f-3122ed66a7f8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}